## Blog Post Assignment 2: EDA
*This blog post will discuss my personal experience with Exploratory Data Analysis (EDA)*  

I certaintly agree with the prompt that EDA is essentially an art or practice. It definitely takes some getting used to and over time I believe it should get easier, or at least, a little more routine. 

I am not super experienced with EDA. With most of my experience coming from college assignments, along with some minor EDA-related projects at work, I still very much feel like I'm getting the hang of it. However, in each instance my overall goal was to identify patterns or better understand the data at hand before advancing to next steps, like building a model or running a statistical test. It would be quite difficult to build a model or test a hypothesis without understanding the data.

My strategy is probably similar to others. I have almost always used R-Studio for EDA with a few EDA encounters ocurring within Excel. The very first thing I do is read the data into R and simply look at the data.frame and try to gauge the datatypes that I have. The goal of this is to figure out if any columns could be factorable/represented on a level-basis or if any columns are binary. If I know that the next step is going to be building some type of model, I will try to identify any potential response/explanatory variables. Knowing this will have an impact on how I plot and visualize my data.

Next, I'll visualize the data with the idea and try to catch data points that could be problematic, or potentially seem erroneous. Using the IQR-Test for outliers or a box plot are my go-to's in this area. I typically do not remove outliers, but, recognize that imputation could be an option if one wanted to remove them. I only remove them in the instance where I know for sure that the data point is an error.

Other plotting, like scatter plots, or correlation plots, are usually what I do next. This can give me a sense of how related variables might be to one another. The ultimate goal here is to find any relationships between variables. This can help us in our next steps. For example, we'd prefer to avoid multicollinearity when building a model.

The methods most important, in my opintion, are the what I do first and last. First, simply viewing the dataset, and last, finding correlations/relationships within the dataset. That isn't to say that searching for outliers, removing errors, and/or cleaning your dataset isn't important, but to me, my beginning and ending steps are what matters most. All in all, EDA is an important part of anything related to data science. Understanding the data you're working with and identifying any potential patterns or relationships will make a vast difference when taking the next steps with the data.
